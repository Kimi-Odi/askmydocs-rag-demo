import io
import os

import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader

from rag_pipeline import (
    build_docs_from_text,
    build_vector_store,
    build_qa_chain,
    save_vector_store,
    load_vector_store,
    get_docs_stats_from_vector_store,
    get_source_names,
    semantic_search,
    summarize_text,
    compare_two_sources,
)

load_dotenv()  # è¼‰å…¥ .env è£¡çš„ OPENAI_API_KEY

st.set_page_config(
    page_title="AskMyDocs â€” AI Document Explorer",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)


# ========= åˆå§‹åŒ– Session State =========

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

if "messages" not in st.session_state:
    st.session_state.messages = []

if "docs_stats" not in st.session_state:
    st.session_state.docs_stats = None

if "temperature" not in st.session_state:
    st.session_state.temperature = 0.2

if "top_k" not in st.session_state:
    st.session_state.top_k = 4

if "show_sources" not in st.session_state:
    st.session_state.show_sources = True

if "persist_enabled" not in st.session_state:
    st.session_state.persist_enabled = False

if "language_mode" not in st.session_state:
    st.session_state.language_mode = "ç¹é«”ä¸­æ–‡"

if "answer_style" not in st.session_state:
    st.session_state.answer_style = "è©³ç´°èªªæ˜"

if "doc_summaries" not in st.session_state:
    # { filename: summary_text }
    st.session_state.doc_summaries = {}


# ========= Sidebarï¼šè¨­å®šèˆ‡å·¥å…· =========

with st.sidebar:
    st.title("âš™ï¸ è¨­å®š")

    st.session_state.temperature = st.slider(
        "LLM æº«åº¦ (temperature)",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.temperature,
        step=0.05,
        help="è¶Šé«˜è¶Šæœ‰å‰µé€ åŠ›ï¼Œä½†å¯èƒ½æ¯”è¼ƒä¸ç©©å®šï¼›è¶Šä½è¶Šä¿å®ˆã€‚",
    )

    st.session_state.top_k = st.slider(
        "æ¯æ¬¡æª¢ç´¢æ–‡ä»¶æ•¸é‡ï¼ˆTop-Kï¼‰",
        min_value=1,
        max_value=8,
        value=st.session_state.top_k,
        step=1,
        help="æ¯æ¬¡å›ç­”æœƒå¾å‘é‡åº«ä¸­å–å‡ºå‰ K å€‹æœ€ç›¸ä¼¼çš„ç‰‡æ®µã€‚",
    )

    st.session_state.language_mode = st.selectbox(
        "å›ç­”èªè¨€",
        ["ç¹é«”ä¸­æ–‡", "English", "ä¸­è‹±é›™èª"],
        index=["ç¹é«”ä¸­æ–‡", "English", "ä¸­è‹±é›™èª"].index(
            st.session_state.language_mode
        ),
    )

    st.session_state.answer_style = st.selectbox(
        "å›ç­”é¢¨æ ¼",
        ["ç²¾ç°¡å›ç­”", "è©³ç´°èªªæ˜", "æ¢åˆ—é‡é»", "è€ƒè©¦è§£é¡Œæ¨¡å¼"],
        index=["ç²¾ç°¡å›ç­”", "è©³ç´°èªªæ˜", "æ¢åˆ—é‡é»", "è€ƒè©¦è§£é¡Œæ¨¡å¼"].index(
            st.session_state.answer_style
        ),
    )

    st.session_state.show_sources = st.checkbox(
        "å›ç­”ä¸‹æ–¹é¡¯ç¤ºåƒè€ƒä¾†æºç‰‡æ®µ & ä¿¡å¿ƒåˆ†æ•¸",
        value=st.session_state.show_sources,
    )

    st.session_state.persist_enabled = st.checkbox(
        "å•Ÿç”¨å‘é‡åº«æŒä¹…åŒ–ï¼ˆå­˜åˆ°æœ¬æ©Ÿ faiss_dbï¼‰",
        value=st.session_state.persist_enabled,
        help="å‹¾é¸å¾Œå»ºç«‹çŸ¥è­˜åº«æ™‚æœƒè‡ªå‹•å„²å­˜ï¼Œä¹‹å¾Œå¯ç›´æ¥å¾ç£ç¢Ÿè¼‰å…¥ã€‚",
    )

    st.markdown("---")

    if st.button("ğŸ§¹ æ¸…ç©ºå°è©±"):
        st.session_state.messages = []
        st.success("å°è©±å·²æ¸…ç©ºã€‚")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡åº«"):
        st.session_state.vector_store = None
        st.session_state.qa_chain = None
        st.session_state.docs_stats = None
        st.session_state.doc_summaries = {}
        st.success("å‘é‡åº«å·²æ¸…ç©ºã€‚")

    if st.button("ğŸ’¾ å¾ç£ç¢Ÿè¼‰å…¥å‘é‡åº« (faiss_db)"):
        try:
            vector_store = load_vector_store("faiss_db")
            st.session_state.vector_store = vector_store
            st.session_state.qa_chain = build_qa_chain(
                vector_store,
                k=st.session_state.top_k,
                temperature=st.session_state.temperature,
            )
            st.session_state.docs_stats = get_docs_stats_from_vector_store(
                vector_store
            )
            st.success("å·²å¾ faiss_db æˆåŠŸè¼‰å…¥å‘é‡åº«ï¼")
        except Exception as e:
            st.error(f"è¼‰å…¥å¤±æ•—ï¼š{e}")

    # ä¸‹è¼‰å°è©±ç´€éŒ„
    if st.session_state.messages:
        md_lines = []
        for m in st.session_state.messages:
            role = "ä½¿ç”¨è€…" if m["role"] == "user" else "AI"
            md_lines.append(f"### {role}\n\n{m['content']}\n")
        md_text = "\n".join(md_lines)
        st.download_button(
            "ğŸ’¾ ä¸‹è¼‰å°è©±ç´€éŒ„ (Markdown)",
            data=md_text,
            file_name="askmydocs_chat.md",
            mime="text/markdown",
        )


# ========= Main å€ï¼šæ¨™é¡Œèˆ‡èªªæ˜ =========

st.markdown(
    """
    <style>
    /* æ•´é«”èƒŒæ™¯ï¼šæ·¡æ·¡çš„è—è‰²ç§‘æŠ€æ„Ÿ */
    .stApp {
        background: radial-gradient(circle at top, #e0f2fe 0, #f9fafb 45%, #f3f4f6 100%);
    }

    /* ä¸»æ¨™é¡Œå¡ç‰‡ */
    .main-header {
        padding: 1.8rem 1rem 1.2rem 1rem;
        border-radius: 18px;
        background: linear-gradient(135deg, #0f4c81 0%, #3a8dde 50%, #6fd3ff 100%);
        color: white;
        text-align: center;
        margin-bottom: 1.2rem;
        box-shadow: 0 12px 30px rgba(10, 40, 80, 0.25);
    }

    .main-header-title {
        font-size: 2rem;
        font-weight: 800;
        margin: 0 0 0.3rem 0;
        letter-spacing: 0.03em;
    }

    .main-header-title span.brand {
        opacity: 0.95;
    }

    .main-header-subtitle {
        font-size: 1rem;
        margin: 0;
        opacity: 0.95;
    }

    .main-header-badge {
        display: inline-block;
        margin-top: 0.7rem;
        padding: 0.25rem 0.85rem;
        border-radius: 999px;
        background: rgba(255,255,255,0.16);
        font-size: 0.85rem;
        backdrop-filter: blur(6px);
    }
    </style>

    <div class="main-header">
        <!-- Logo å€å¡Šï¼šç”¨ emoji ç•¶ç°¡å–® Logo -->
        <div class="main-header-title">
            ğŸ” <span class="brand">AskMyDocs â€” AI Document Explorer</span>
        </div>
        <p class="main-header-subtitle">
            Upload Â· Search Â· Understand &nbsp; | &nbsp; Powered by Retrieval-Augmented Generation
        </p>
        <div class="main-header-badge">
            Multi-file RAG Â· Vector DB Â· Source Highlight Â· Persistent Knowledge Base
        </div>
    </div>
    """,
    unsafe_allow_html=True,
)

st.write(
    "ä¸Šå‚³ä¸€å€‹æˆ–å¤šå€‹ PDF / TXT æª”æ¡ˆï¼Œç³»çµ±æœƒç‚ºä½ çš„æ–‡ä»¶å»ºç«‹å‘é‡åŒ–çŸ¥è­˜åº«ï¼Œ"
    "ä¹‹å¾Œä½ å¯ä»¥åƒå•äººä¸€æ¨£ï¼Œç›´æ¥ç”¨è‡ªç„¶èªè¨€å‘æ–‡ä»¶æå•ã€‚"
)



# ========= æª”æ¡ˆä¸Šå‚³èˆ‡å‘é‡åº«å»ºç«‹ =========

uploaded_files = st.file_uploader(
    "ä¸Šå‚³ä¸€å€‹æˆ–å¤šå€‹ PDF / TXT æª”æ¡ˆ",
    type=["pdf", "txt"],
    accept_multiple_files=True,
)

if uploaded_files and st.button("ğŸ“š å»ºç«‹ / æ›´æ–°çŸ¥è­˜åº«"):
    all_docs = []
    doc_summaries = {}

    # æ±ºå®šæ‘˜è¦èªè¨€ï¼ˆè½‰æˆ 'zh' / 'en' / 'bi'ï¼‰
    lang_code = {
        "ç¹é«”ä¸­æ–‡": "zh",
        "English": "en",
        "ä¸­è‹±é›™èª": "bi",
    }.get(st.session_state.language_mode, "zh")

    for f in uploaded_files:
        file_bytes = f.read()
        text = ""

        if f.type == "application/pdf":
            try:
                pdf_reader = PdfReader(io.BytesIO(file_bytes))
                for page in pdf_reader.pages:
                    page_text = page.extract_text() or ""
                    text += page_text + "\n"
            except Exception as e:
                st.error(f"è®€å– PDF æª”æ¡ˆ {f.name} å¤±æ•—ï¼š{e}")
                continue
        elif f.type == "text/plain":
            try:
                text = file_bytes.decode("utf-8", errors="ignore")
            except Exception as e:
                st.error(f"è®€å–æ–‡å­—æª” {f.name} å¤±æ•—ï¼š{e}")
                continue

        if not text.strip():
            st.warning(f"æª”æ¡ˆ {f.name} çœ‹èµ·ä¾†æ²’æœ‰å¯è®€å–çš„æ–‡å­—å…§å®¹ï¼Œå·²ç•¥éã€‚")
            continue

        # æ¯å€‹æª”æ¡ˆçš„ chunk
        docs = build_docs_from_text(text, source_name=f.name)
        all_docs.extend(docs)

        # è‡ªå‹•æ‘˜è¦
        with st.spinner(f"æ­£åœ¨ç‚º {f.name} ç”¢ç”Ÿæ‘˜è¦..."):
            try:
                summary = summarize_text(text, language_mode=lang_code)
                doc_summaries[f.name] = summary
            except Exception as e:
                doc_summaries[f.name] = f"ç”¢ç”Ÿæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

    if not all_docs:
        st.error("æ²’æœ‰æˆåŠŸæ“·å–åˆ°ä»»ä½•æ–‡å­—å…§å®¹ï¼Œè«‹æª¢æŸ¥ä¸Šå‚³çš„æª”æ¡ˆã€‚")
    else:
        with st.spinner("æ­£åœ¨å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ˆEmbedding + Indexingï¼‰..."):
            vector_store = build_vector_store(all_docs)
            st.session_state.vector_store = vector_store
            st.session_state.qa_chain = build_qa_chain(
                vector_store,
                k=st.session_state.top_k,
                temperature=st.session_state.temperature,
            )
            st.session_state.docs_stats = get_docs_stats_from_vector_store(
                vector_store
            )
            st.session_state.doc_summaries = doc_summaries

            if st.session_state.persist_enabled:
                try:
                    save_vector_store(vector_store, "faiss_db")
                    st.info("å‘é‡åº«å·²å­˜è‡³æœ¬æ©Ÿè³‡æ–™å¤¾ï¼šfaiss_db")
                except Exception as e:
                    st.error(f"å„²å­˜å‘é‡åº«å¤±æ•—ï¼š{e}")

        st.success("âœ… çŸ¥è­˜åº«å»ºç«‹ / æ›´æ–°å®Œæˆï¼å¯ä»¥é–‹å§‹æå•ã€‚")


# ========= æ–‡ä»¶çµ±è¨ˆè³‡è¨Š & æ‘˜è¦ =========

if st.session_state.docs_stats:
    stats = st.session_state.docs_stats
    st.markdown(
        f"""**ç›®å‰å‘é‡åº«çµ±è¨ˆï¼š**  
- Chunk æ•¸é‡ï¼š`{stats["num_docs"]}`  
- ç¸½å­—å…ƒæ•¸ï¼šç´„ `{stats["total_chars"]}`  
- å¹³å‡æ¯å€‹ chunk å­—å…ƒæ•¸ï¼šç´„ `{int(stats["avg_chars"])}`  
"""
    )

    if stats.get("per_source"):
        st.markdown("**å„æª”æ¡ˆ chunk æ•¸é‡ï¼š**")
        for src, cnt in stats["per_source"].items():
            st.markdown(f"- `{src}`ï¼š{cnt} chunks")

if st.session_state.doc_summaries:
    with st.expander("ğŸ“„ æ–‡ä»¶æ‘˜è¦ï¼ˆAuto Summaryï¼‰"):
        for fname, summary in st.session_state.doc_summaries.items():
            st.markdown(f"### ğŸ“˜ {fname}")
            st.write(summary)


st.divider()

# ========= Semantic Search & æ–‡ä»¶æ¯”è¼ƒ =========

if st.session_state.vector_store is not None:
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ğŸ” Semantic Searchï¼ˆåªæª¢ç´¢ï¼Œä¸ç”¢ç”Ÿå›ç­”ï¼‰")
        semantic_query = st.text_input("è¼¸å…¥æƒ³åœ¨æ–‡ä»¶ä¸­æœå°‹çš„å…§å®¹ï¼ˆé—œéµå­—æˆ–è‡ªç„¶èªè¨€ï¼‰", key="semantic_q")
        if st.button("åŸ·è¡Œæœå°‹", key="semantic_btn") and semantic_query:
            with st.spinner("æœå°‹ä¸­â€¦"):
                try:
                    results = semantic_search(
                        st.session_state.vector_store, semantic_query, k=5
                    )
                    if not results:
                        st.info("æ‰¾ä¸åˆ°ç›¸é—œç‰‡æ®µã€‚")
                    else:
                        for i, (doc, score) in enumerate(results, start=1):
                            meta = doc.metadata or {}
                            src = meta.get("source", "unknown")
                            cid = meta.get("chunk_id", "?")
                            st.markdown(
                                f"**çµæœ {i}** â€“ æª”æ¡ˆï¼š`{src}`ï¼Œchunkï¼š`{cid}`ï¼Œscoreï¼š`{score:.4f}`"
                            )
                            st.write(doc.page_content)
                except Exception as e:
                    st.error(f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    with col2:
        st.markdown("### ğŸ“Š æ–‡ä»¶æ¯”è¼ƒï¼ˆDocument Compareï¼‰")
        try:
            sources = get_source_names(st.session_state.vector_store)
        except Exception as e:
            sources = []
            st.error(f"å–å¾—ä¾†æºæª”åå¤±æ•—ï¼š{e}")

        if len(sources) >= 2:
            src_a = st.selectbox("é¸æ“‡æ–‡ä»¶ A", sources, key="cmp_a")
            src_b = st.selectbox("é¸æ“‡æ–‡ä»¶ B", sources, key="cmp_b", index=1)
            if st.button("æ¯”è¼ƒé€™å…©ä»½æ–‡ä»¶", key="cmp_btn"):
                lang_code_cmp = {
                    "ç¹é«”ä¸­æ–‡": "zh",
                    "English": "en",
                    "ä¸­è‹±é›™èª": "bi",
                }.get(st.session_state.language_mode, "zh")
                with st.spinner("AI æ­£åœ¨æ¯”è¼ƒå…©ä»½æ–‡ä»¶â€¦"):
                    try:
                        cmp_result = compare_two_sources(
                            st.session_state.vector_store,
                            src_a,
                            src_b,
                            language_mode=lang_code_cmp,
                        )
                        st.markdown("#### ğŸ“ æ¯”è¼ƒçµæœ")
                        st.write(cmp_result)
                    except Exception as e:
                        st.error(f"æ¯”è¼ƒæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        else:
            st.info("ç›®å‰åªæœ‰ä¸€ä»½æˆ–æ²’æœ‰æ–‡ä»¶ï¼Œç„¡æ³•æ¯”è¼ƒã€‚")


st.divider()

# ========= èŠå¤©å€ï¼ˆRAG å•ç­”ï¼‰ =========

if st.session_state.qa_chain is None:
    st.info("è«‹å…ˆä¸Šå‚³æª”æ¡ˆä¸¦å»ºç«‹çŸ¥è­˜åº«ï¼Œæˆ–å¾ Sidebar è¼‰å…¥æ—¢æœ‰å‘é‡åº«ã€‚")
else:
    # å…ˆæŠŠæ­·å²è¨Šæ¯ç•«å‡ºä¾†
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # ä½¿ç”¨è€…è¼¸å…¥
    user_question = st.chat_input("è«‹è¼¸å…¥ä½ æƒ³å•æ–‡ä»¶çš„å•é¡Œâ€¦")

    if user_question:
        # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.write(user_question)

        # å‘¼å« RAG Chain
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­â€¦"):
                try:
                    lang_code = {
                        "ç¹é«”ä¸­æ–‡": "zh",
                        "English": "en",
                        "ä¸­è‹±é›™èª": "bi",
                    }.get(st.session_state.language_mode, "zh")

                    style_code = {
                        "ç²¾ç°¡å›ç­”": "concise",
                        "è©³ç´°èªªæ˜": "detailed",
                        "æ¢åˆ—é‡é»": "bullets",
                        "è€ƒè©¦è§£é¡Œæ¨¡å¼": "exam",
                    }.get(st.session_state.answer_style, "detailed")

                    result = st.session_state.qa_chain(
                        {
                            "query": user_question,
                            "language_mode": lang_code,
                            "answer_style": style_code,
                        }
                    )
                    answer = result["result"]
                    sources = result.get("source_documents", [])
                    doc_scores = result.get("doc_scores", [])
                except Exception as e:
                    answer = f"å›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
                    sources = []
                    doc_scores = []

                st.write(answer)
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )

                # é¡¯ç¤ºä¾†æºç‰‡æ®µ + ä¿¡å¿ƒåˆ†æ•¸
                if st.session_state.show_sources and sources:
                    with st.expander("ğŸ“ åƒè€ƒä¾†æºç‰‡æ®µ & ä¿¡å¿ƒåˆ†æ•¸"):
                        for i, doc in enumerate(sources, start=1):
                            meta = doc.metadata or {}
                            src = meta.get("source", "unknown")
                            cid = meta.get("chunk_id", "?")

                            score_info = ""
                            if i - 1 < len(doc_scores):
                                ds = doc_scores[i - 1]
                                score_info = (
                                    f" | score: {ds['score']:.4f} | "
                                    f"confidence: {ds['confidence']:.2f}"
                                )

                            st.markdown(
                                f"**ä¾†æº {i}** â€“ æª”æ¡ˆï¼š`{src}`ï¼Œchunkï¼š`{cid}`{score_info}"
                            )
                            st.write(doc.page_content)
                            st.caption(str(meta))
